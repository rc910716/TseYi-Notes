## Task03 深度学习及推理引擎

![模型优化器和推理引擎](./images/task02/dldt-offline-process.png)

### 1 模型优化器简介

- DLDT 的两级优化：模型优化器、推理引擎
- 模型优化器：跨平台的命令行工具，将各种深度学习框架模型转换 IR 文件，并使用推理引擎对其进行读取、加载和推理
- IR 文件：包含了 xml 文件（网络的拓扑结构）和 bin 文件（权重和偏差）
- 特点：生成的 IR 文件可在 AI 应用的推理过程中被反复使用，模型的准确度会有轻度下降，但性能会变得更强
- 功能：
    1. 将模型转换成 IR 文件
    2. 将模型网络操作映射到支持的库、内核或层
    3. 预处理操作
    4. 网络优化操作：调整神经网络输入批次大小、输入大小
    5. 调整输入的数据格式，支持 FP32、FP16、Int8
    6. 剪辑模型网络
    7. 支持构建自定义层

### 2 推理引擎

#### 2.1 推理引擎优化

- 针对特定执行设备对模型的优化
- 使用灵活的插件架构去执行环境配置
- 网络映射到库单元，并通过网络发送到硬件插件，进行特定优化，包括网络级优化、内存级优化、内核级优化

#### 2.2 推理引擎 API

- 在所有英特尔架构的硬件中实施推理的一套简单而且统一的 API
- 特殊插件架构支持优化推理性能和内存使用
- 主要使用 C++ 语言
- API 代码接口：
  1. IECore 类：不指定任何特定的设备，只有加载网络时指定设备
  2. HETERO 插件：可以将不支持层的执行回退到其他设备
  3. MULTI 插件：支持在不同的设备上运行每个推理调用，完整利用系统中的所有设备
- 具备实际监控运行时性能计数器
- 可以感知设备的连接状态情况，还可以获取实际物理状况的实时指示

### 3 性能评估与硬件选择

- 推理引擎工作流：声明 IE 对象，读取一个神经网络，加载网络到插件中，按照网络输入和输出的实际尺寸分配输入和输出 blob，开始进行推理
- 影响模型性能的因素
    1. 吞吐量：表示神经网络在一秒钟内可以处理的帧数，单位是“每秒推理数”
    2. 延迟：表示从数据开始分析到结果可以被读取的时间，单位是“毫秒”
    3. 效率：单位是“瓦特”或“单位价格的每秒帧数”，基于系统的功耗或价格因素
- 影响特定设备的神经网络性能的因素
    1. 神经网络的拓扑或模型参数
    2. 异构设备：CPU、GPU、FPGA、AI 加速器（VPU 视觉处理单元），例如构建应用，先确定运行神经网络的地方，在 Video 计算设备上进行推理，在 GPU 上运行视频处理，在 CPU 上运行其他任务和逻辑
    3. 模型精度（数据格式）：在生成 IR 文件之后进行校准流程，其优势是为特定设备选择最佳数据格式、优化运行时间和内存、在 DL Boost 使用 int8 格式提升性能
    4. 批处理：增加批处理层次，提高计算效率
    5. 异步执行：提高每一帧的异步处理
    6. CPU 的 Throughput 模式：通过监控并行数，控制 CPU 资源的智能分配，并分配至多个推理请求，CPU 核心越多，功能的效率越高

### 4 总结

&emsp;&emsp; 本次任务，主要介绍模型优化器、推理引擎和模型性能影响因素：

1. 模型优化器将各种深度学习框架模型转换 IR 文件
2. 推理引擎读取 IR 文件进行读取、加载和推理
3. 影响模型性能的主要因素包括吞吐量、延迟和效率
